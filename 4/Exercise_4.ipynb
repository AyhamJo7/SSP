{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Signal Processing - Exercise 4\n",
    "## Vocoder and Quantization\n",
    "\n",
    "In this exercise, we will build our own LPC-vocoder. We'll implement the analysis stage to extract parameters from a speech signal, then use these parameters in the synthesis stage to reconstruct the speech. Finally, we'll explore quantization of these parameters for efficient data transmission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import sounddevice as sd\n",
    "from scipy.signal import lfilter\n",
    "from scipy.linalg import solve_toeplitz\n",
    "\n",
    "# Import the filter_adaptively function\n",
    "from filteradaptively import filter_adaptively\n",
    "\n",
    "# Import LPC tools\n",
    "from lpctools import poly2rc, rc2lar, lar2rc, rc2poly\n",
    "\n",
    "# Set up plotting parameters\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LPC-Vocoder: Analysis\n",
    "\n",
    "### 2.1 Segmentation\n",
    "\n",
    "First, we'll load the speech signal and segment it into overlapping frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the speech signal\n",
    "x, fs = librosa.load(\"female8khz.wav\", sr=None)\n",
    "\n",
    "# Print basic information\n",
    "print(f\"Loaded female8khz.wav with sampling rate: {fs} Hz\")\n",
    "print(f\"Signal duration: {len(x)/fs:.2f} seconds\")\n",
    "print(f\"Signal shape: {x.shape}\")\n",
    "\n",
    "# Create time vector for plotting\n",
    "time = np.arange(len(x)) / fs\n",
    "\n",
    "# Plot the waveform\n",
    "plt.figure()\n",
    "plt.plot(time, x)\n",
    "plt.title('Speech Signal Waveform')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the windowing function from Exercise 1\n",
    "def my_windowing(v_signal: np.ndarray, sampling_rate: int, frame_length_ms: int, frame_shift_ms: int):\n",
    "    \"\"\"\n",
    "    Splits the signal into overlapping frames.\n",
    "\n",
    "    Parameters:\n",
    "        v_signal         : Input signal (1D numpy array)\n",
    "        sampling_rate    : Sampling rate in Hz\n",
    "        frame_length_ms  : Frame length in milliseconds\n",
    "        frame_shift_ms   : Frame shift (hop size) in milliseconds\n",
    "\n",
    "    Returns:\n",
    "        m_frames         : 2D array with one frame per row\n",
    "        v_time_frame     : Time instants (center) of each frame\n",
    "    \"\"\"\n",
    "    frame_length = int((frame_length_ms / 1000) * sampling_rate)\n",
    "    frame_shift = int((frame_shift_ms / 1000) * sampling_rate)\n",
    "    \n",
    "    # Calculate the number of frames that fit into the signal\n",
    "    num_frames = 1 + (len(v_signal) - frame_length) // frame_shift\n",
    "\n",
    "    # Initialize output arrays\n",
    "    m_frames = np.zeros((num_frames, frame_length))\n",
    "    v_time_frame = np.zeros(num_frames)\n",
    "\n",
    "    for i in range(num_frames):\n",
    "        start = i * frame_shift\n",
    "        end = start + frame_length\n",
    "        m_frames[i] = v_signal[start:end]\n",
    "        v_time_frame[i] = (start + frame_length // 2) / sampling_rate\n",
    "\n",
    "    return m_frames, v_time_frame\n",
    "\n",
    "# Segment the signal with 32 ms frames and 8 ms shift\n",
    "frame_length_ms = 32\n",
    "frame_shift_ms = 8\n",
    "m_frames, v_time_frame = my_windowing(x, fs, frame_length_ms, frame_shift_ms)\n",
    "\n",
    "print(f\"Number of frames: {len(m_frames)}\")\n",
    "print(f\"Frame length: {m_frames.shape[1]} samples ({frame_length_ms} ms)\")\n",
    "print(f\"Frame shift: {int((frame_shift_ms / 1000) * fs)} samples ({frame_shift_ms} ms)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why do we segment the signal prior to analysis instead of processing the whole signal at once?**\n",
    "\n",
    "We segment the speech signal prior to analysis for several important reasons:\n",
    "\n",
    "1. **Stationarity**: Speech signals are non-stationary, meaning their statistical properties change over time. By segmenting the signal into short frames (typically 20-40 ms), we can assume that within each frame, the signal is approximately stationary, which is a key assumption for many speech processing techniques like LPC analysis.\n",
    "\n",
    "2. **Time-varying characteristics**: The vocal tract shape and excitation source change continuously during speech production. Segmentation allows us to track these changes over time and model them appropriately.\n",
    "\n",
    "3. **Computational efficiency**: Processing the entire signal at once would be computationally intensive and might not capture the time-varying nature of speech effectively.\n",
    "\n",
    "4. **Parameter extraction**: Many speech parameters (like pitch, voicing, formants) change over time. Segmentation allows us to extract these parameters as functions of time.\n",
    "\n",
    "**Is a segment length of 32 ms appropriate? Why or why not?**\n",
    "\n",
    "A segment length of 32 ms is appropriate for speech analysis for the following reasons:\n",
    "\n",
    "1. **Stationarity**: 32 ms is short enough that the speech signal can be considered approximately stationary within each frame, yet long enough to capture meaningful spectral information.\n",
    "\n",
    "2. **Pitch period coverage**: For typical human speech with fundamental frequencies between 80-400 Hz, a 32 ms window can capture 2-12 pitch periods, which is sufficient for reliable pitch estimation and spectral analysis.\n",
    "\n",
    "3. **Frequency resolution**: A 32 ms window provides adequate frequency resolution (approximately 31.25 Hz for an 8 kHz sampling rate) to distinguish between formants in the spectrum.\n",
    "\n",
    "4. **Standard practice**: This duration is commonly used in speech processing applications and has been empirically shown to work well for various speech analysis tasks.\n",
    "\n",
    "The 8 ms frame shift (75% overlap) ensures smooth transitions between frames while still capturing the temporal dynamics of the speech signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Signal power\n",
    "\n",
    "Now we'll compute the signal power for each frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_power(segment):\n",
    "    \"\"\"\n",
    "    Compute the signal power of a segment.\n",
    "    \n",
    "    Parameters:\n",
    "        segment: Input signal segment\n",
    "        \n",
    "    Returns:\n",
    "        power: Signal power\n",
    "    \"\"\"\n",
    "    return np.mean(segment**2)\n",
    "\n",
    "# Compute power for each frame\n",
    "v_power = np.array([compute_power(frame) for frame in m_frames])\n",
    "\n",
    "# Compute standard deviation (sqrt of power)\n",
    "v_std = np.sqrt(v_power)\n",
    "\n",
    "# Plot the waveform and standard deviation\n",
    "plt.figure()\n",
    "plt.plot(time, x, 'b', alpha=0.5, label='Speech signal')\n",
    "plt.plot(v_time_frame, v_std, 'r', linewidth=2, label='Standard deviation')\n",
    "plt.title('Speech Signal and Standard Deviation')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Voiced / unvoiced decision\n",
    "\n",
    "Next, we'll implement a function to determine if a frame is voiced or unvoiced based on zero-crossing rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_voiced(segment, threshold=0.2):\n",
    "    \"\"\"\n",
    "    Determine if a segment is voiced or unvoiced based on zero-crossing rate.\n",
    "    \n",
    "    Parameters:\n",
    "        segment: Input signal segment\n",
    "        threshold: Threshold for zero-crossing rate decision\n",
    "        \n",
    "    Returns:\n",
    "        1 if voiced, 0 if unvoiced\n",
    "    \"\"\"\n",
    "    # Count zero crossings\n",
    "    # A zero crossing occurs when the sign changes between consecutive samples\n",
    "    # We can detect this by multiplying adjacent samples and checking if the result is negative\n",
    "    zero_crossings = np.sum(segment[:-1] * segment[1:] < 0)\n",
    "    \n",
    "    # Normalize by segment length\n",
    "    zero_crossing_rate = zero_crossings / (len(segment) - 1)\n",
    "    \n",
    "    # Apply threshold\n",
    "    return 1 if zero_crossing_rate < threshold else 0\n",
    "\n",
    "# Compute voiced/unvoiced decision for each frame\n",
    "v_voiced = np.apply_along_axis(is_voiced, 1, m_frames)\n",
    "\n",
    "# Plot the waveform and voiced/unvoiced decision\n",
    "plt.figure()\n",
    "plt.plot(time, x, 'b', alpha=0.5, label='Speech signal')\n",
    "plt.plot(v_time_frame, v_voiced * max(abs(x)), 'r', linewidth=2, label='Voiced (1) / Unvoiced (0)')\n",
    "plt.title('Speech Signal and Voiced/Unvoiced Decision')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain differences in the creation of speech for voiced and unvoiced sounds.**\n",
    "\n",
    "Voiced and unvoiced sounds are produced through fundamentally different mechanisms in the human speech production system:\n",
    "\n",
    "**Voiced sounds:**\n",
    "- Produced when the vocal folds (cords) vibrate periodically\n",
    "- Air from the lungs causes the vocal folds to open and close at a regular rate\n",
    "- This creates a quasi-periodic excitation signal (a series of pulses)\n",
    "- The rate of vibration determines the fundamental frequency (pitch) of the voice\n",
    "- Examples: vowels (/a/, /e/, /i/, /o/, /u/), nasals (/m/, /n/), and some consonants (/b/, /d/, /g/, /v/, /z/)\n",
    "- Characterized by clear periodicity in the waveform and harmonic structure in the spectrum\n",
    "\n",
    "**Unvoiced sounds:**\n",
    "- Produced without vocal fold vibration\n",
    "- Air flows through a constriction in the vocal tract, creating turbulence\n",
    "- This turbulence generates noise-like excitation\n",
    "- Examples: fricatives (/f/, /s/, /sh/, /h/), plosives (/p/, /t/, /k/)\n",
    "- Characterized by random-like patterns in the waveform and a more flat, noise-like spectrum\n",
    "\n",
    "**Why might the number of zero crossings provide valuable information for the voiced/unvoiced decision?**\n",
    "\n",
    "The number of zero crossings is a useful indicator for voiced/unvoiced decision because:\n",
    "\n",
    "1. **Frequency content**: Unvoiced sounds typically have more high-frequency energy, which results in more rapid fluctuations in the waveform and consequently more zero crossings.\n",
    "\n",
    "2. **Periodicity vs. randomness**: Voiced sounds have a more regular, periodic structure with fewer zero crossings per unit time, while unvoiced sounds have a more random, noise-like structure with more frequent zero crossings.\n",
    "\n",
    "3. **Simplicity**: Zero-crossing rate is computationally simple to calculate, making it an efficient feature for real-time processing.\n",
    "\n",
    "4. **Robustness**: It's relatively robust to amplitude variations, as it only depends on the sign of the signal, not its absolute value.\n",
    "\n",
    "**How can you efficiently detect a zero crossing between two samples?**\n",
    "\n",
    "A zero crossing occurs when consecutive samples have opposite signs. In Python, we can efficiently detect zero crossings by:\n",
    "\n",
    "1. Multiplying adjacent samples: If the product is negative, the samples have opposite signs, indicating a zero crossing.\n",
    "2. Using vectorized operations: `np.sum(segment[:-1] * segment[1:] < 0)` counts all instances where adjacent samples have opposite signs.\n",
    "\n",
    "This approach is more efficient than using loops and explicit sign checking, as it leverages NumPy's vectorized operations.\n",
    "\n",
    "**In general: are all speech sounds either voiced or unvoiced? Can you think of other speech sounds?**\n",
    "\n",
    "Not all speech sounds are purely voiced or unvoiced. There are several categories that don't fit neatly into this binary classification:\n",
    "\n",
    "1. **Mixed excitation sounds**: Some sounds have both voiced and unvoiced components simultaneously. Examples include voiced fricatives like /v/, /z/, /zh/ where the vocal folds vibrate while turbulent airflow is created at a constriction.\n",
    "\n",
    "2. **Plosives/stops**: Sounds like /b/, /d/, /g/, /p/, /t/, /k/ have complex temporal patterns with a silence period (closure), followed by a burst (release). Their voiced/unvoiced nature can change during articulation.\n",
    "\n",
    "3. **Transitions**: The boundaries between phonemes often contain transitional regions that are neither fully voiced nor unvoiced.\n",
    "\n",
    "4. **Whispered speech**: In whispered speech, traditionally voiced sounds are produced without vocal fold vibration but maintain some characteristics of voiced sounds.\n",
    "\n",
    "5. **Creaky voice/vocal fry**: A phonation type with irregular vocal fold vibration that doesn't fit the typical voiced pattern.\n",
    "\n",
    "These complexities make speech analysis challenging and often require more sophisticated models than a simple binary voiced/unvoiced decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Fundamental frequency estimation\n",
    "\n",
    "Now we'll implement a function to estimate the fundamental frequency (f0) for each frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acf_matrix(m_frames):\n",
    "    \"\"\"\n",
    "    Compute the autocorrelation function (ACF) for each frame.\n",
    "    Returns only the positive-lag part of the ACF (lags >= 0).\n",
    "\n",
    "    Parameters:\n",
    "        m_frames : 2D array of frames (each row is a frame)\n",
    "\n",
    "    Returns:\n",
    "        acf_matrix : 2D array with ACFs (lags >= 0)\n",
    "    \"\"\"\n",
    "    num_frames, frame_len = m_frames.shape\n",
    "    acf_matrix = np.zeros((num_frames, frame_len))\n",
    "\n",
    "    for i in range(num_frames):\n",
    "        frame = m_frames[i]\n",
    "        # Full ACF via convolution of frame with time-reversed version\n",
    "        acf_full = np.convolve(frame, frame[::-1], mode='full')\n",
    "        # Keep only lags >= 0 (centered at frame_len - 1)\n",
    "        acf_matrix[i] = acf_full[frame_len - 1:]\n",
    "\n",
    "    return acf_matrix\n",
    "\n",
    "def estimate_f0(m_frames, fs, fmin=80, fmax=400):\n",
    "    \"\"\"\n",
    "    Estimate the fundamental frequency from frames using autocorrelation.\n",
    "    \n",
    "    Parameters:\n",
    "        m_frames: Matrix of signal frames\n",
    "        fs: Sampling frequency in Hz\n",
    "        fmin: Minimum allowed frequency in Hz\n",
    "        fmax: Maximum allowed frequency in Hz\n",
    "        \n",
    "    Returns:\n",
    "        v_f0_estimates: Vector of fundamental frequency estimates in Hz\n",
    "    \"\"\"\n",
    "    # Compute ACF for all frames\n",
    "    acf_matrix = compute_acf_matrix(m_frames)\n",
    "    \n",
    "    num_frames, frame_len = acf_matrix.shape\n",
    "    v_f0_estimates = np.zeros(num_frames)\n",
    "    \n",
    "    # Convert frequency range to lag range\n",
    "    min_lag = int(fs / fmax)\n",
    "    max_lag = int(fs / fmin)\n",
    "    \n",
    "    for i in range(num_frames):\n",
    "        # Only estimate f0 for voiced frames\n",
    "        if v_voiced[i] == 1:\n",
    "            acf = acf_matrix[i]\n",
    "            # Search for peak in valid lag range\n",
    "            search_region = acf[min_lag:max_lag]\n",
    "            if len(search_region) > 0:\n",
    "                peak_index = np.argmax(search_region) + min_lag\n",
    "                v_f0_estimates[i] = fs / peak_index\n",
    "        else:\n",
    "            v_f0_estimates[i] = 0  # Set f0 to 0 for unvoiced frames\n",
    "    \n",
    "    return v_f0_estimates\n",
    "\n",
    "# Estimate fundamental frequency\n",
    "v_f0 = estimate_f0(m_frames, fs)\n",
    "\n",
    "# Compute spectrogram for visualization\n",
    "def compute_stft(signal, fs, frame_length_ms=32, frame_shift_ms=8, window='hann'):\n",
    "    \"\"\"\n",
    "    Compute the Short-Time Fourier Transform (STFT) of a signal.\n",
    "    \n",
    "    Parameters:\n",
    "        signal: Input signal\n",
    "        fs: Sampling frequency in Hz\n",
    "        frame_length_ms: Frame length in milliseconds\n",
    "        frame_shift_ms: Frame shift in milliseconds\n",
    "        window: Window type\n",
    "        \n",
    "    Returns:\n",
    "        stft_magnitude: Magnitude of STFT\n",
    "        time_vec: Time vector\n",
    "        freq_vec: Frequency vector\n",
    "    \"\"\"\n",
    "    n_fft = int((frame_length_ms / 1000) * fs)\n",
    "    hop_length = int((frame_shift_ms / 1000) * fs)\n",
    "    \n",
    "    # Compute STFT\n",
    "    stft = librosa.stft(signal, n_fft=n_fft, hop_length=hop_length, window=window)\n",
    "    stft_magnitude = np.abs(stft)\n",
    "    \n",
    "    # Create time and frequency vectors\n",
    "    time_vec = librosa.times_like(stft_magnitude, sr=fs, hop_length=hop_length)\n",
    "    freq_vec = librosa.fft_frequencies(sr=fs, n_fft=n_fft)\n",
    "    \n",
    "    return stft_magnitude, time_vec, freq_vec\n",
    "\n",
    "# Compute spectrogram\n",
    "stft_magnitude, stft_time, stft_freq = compute_stft(x, fs)\n",
    "\n",
    "# Plot fundamental frequency estimate with spectrogram\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot spectrogram\n",
    "plt.pcolormesh(stft_time, stft_freq, 20 * np.log10(stft_magnitude + 1e-10), shading='gouraud', cmap='viridis')\n",
    "plt.colorbar(label='Magnitude (dB)')\n",
    "\n",
    "# Plot fundamental frequency\n",
    "plt.plot(v_time_frame, v_f0, 'r.', markersize=5, label='Estimated f0')\n",
    "\n",
    "plt.title('Spectrogram and Fundamental Frequency Estimate')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.ylim(0, 1000)  # Limit y-axis to focus on speech frequencies\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Linear prediction coefficients / linear predictive coding\n",
    "\n",
    "Now we'll compute the Linear Prediction Coefficients (LPCs) for each frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lpc(m_frames, M):\n",
    "    \"\"\"\n",
    "    Compute Linear Prediction coefficients using autocorrelation method.\n",
    "    \n",
    "    Parameters:\n",
    "        m_frames: Matrix of signal frames\n",
    "        M: Order of the LP filter\n",
    "        \n",
    "    Returns:\n",
    "        m_lpc: Matrix of LP coefficients for each frame\n",
    "    \"\"\"\n",
    "    num_frames = m_frames.shape[0]\n",
    "    m_lpc = np.zeros((num_frames, M))\n",
    "    \n",
    "    for i in range(num_frames):\n",
    "        frame = m_frames[i]\n",
    "        \n",
    "        # Compute autocorrelation\n",
    "        r = np.correlate(frame, frame, mode='full')\n",
    "        # Extract the positive lags (center to right half)\n",
    "        r = r[len(frame)-1:len(frame)+M+1]\n",
    "        \n",
    "        # Solve the Toeplitz system\n",
    "        a = solve_toeplitz(r[:M], r[1:M+1])\n",
    "        \n",
    "        # Store the LP coefficients\n",
    "        m_lpc[i] = a\n",
    "    \n",
    "    return m_lpc\n",
    "\n",
    "# Choose LP model order\n",
    "M = 10\n",
    "\n",
    "# Compute LPCs for all frames\n",
    "m_lpc = compute_lpc(m_frames, M)\n",
    "\n",
    "print(f\"LPC matrix shape: {m_lpc.shape}\")\n",
    "print(f\"First frame LPCs: {m_lpc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shortly outline the Source-Filter-Model for speech production.**\n",
    "\n",
    "The Source-Filter Model is a fundamental concept in speech processing that describes speech production as two independent components:\n",
    "\n",
    "1. **Source (Excitation)**: \n",
    "   - For voiced sounds: Quasi-periodic pulses generated by the vibrating vocal folds\n",
    "   - For unvoiced sounds: Noise-like turbulence created by airflow through constrictions\n",
    "   - The source determines whether the sound is voiced or unvoiced and, for voiced sounds, the fundamental frequency (pitch)\n",
    "\n",
    "2. **Filter (Vocal Tract)**:\n",
    "   - The vocal tract (throat, mouth, nasal cavity) acts as an acoustic filter\n",
    "   - It shapes the spectrum of the source signal by amplifying certain frequencies (formants) and attenuating others\n",
    "   - The filter characteristics depend on the shape and configuration of the vocal tract\n",
    "   - This filter can be modeled as an all-pole filter using Linear Prediction Coefficients (LPCs)\n",
    "\n",
    "In this model, speech production is mathematically represented as the convolution of the source signal with the impulse response of the vocal tract filter. In the frequency domain, this corresponds to the multiplication of the source spectrum with the vocal tract transfer function.\n",
    "\n",
    "The beauty of this model is that it separates the two main aspects of speech: the source (which carries information about voicing and pitch) and the filter (which carries information about the phonetic content). This separation forms the basis for many speech processing applications, including vocoders, speech synthesis, and speech coding.\n",
    "\n",
    "**Choose a suitable model order M for linear prediction for a signal with an audio bandwidth of 4 kHz. Give reasons for your choice.**\n",
    "\n",
    "For a speech signal with an audio bandwidth of 4 kHz, a suitable model order (M) for linear prediction would be around 8-12, with 10 being a common choice. Here's the reasoning:\n",
    "\n",
    "1. **Rule of thumb**: A common rule of thumb is to use 4 + (fs/1000) coefficients, where fs is the sampling frequency in Hz. For an 8 kHz sampling rate (which gives a 4 kHz bandwidth), this would be 4 + 8 = 12 coefficients.\n",
    "\n",
    "2. **Formant modeling**: Each formant (resonance peak) in the speech spectrum requires approximately 2 poles to model. Human speech typically has 4-5 significant formants within the 4 kHz bandwidth. Therefore, 8-10 coefficients would be needed to model these formants adequately.\n",
    "\n",
    "3. **Spectral resolution**: The model order determines the spectral resolution of the LP analysis. With a 4 kHz bandwidth, we need enough coefficients to capture the important spectral features without overfitting to noise or harmonics.\n",
    "\n",
    "4. **Computational efficiency**: Higher orders increase computational complexity without necessarily improving performance. An order of 10 provides a good balance between model accuracy and computational efficiency.\n",
    "\n",
    "Based on these considerations, I've chosen M = 10 for this exercise, which should adequately capture the vocal tract characteristics for our 8 kHz sampled speech signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LPC-Vocoder: Synthesis\n",
    "\n",
    "Now that we have extracted all the necessary parameters from the speech signal, we'll use them to synthesize speech using the LPC-vocoder. We'll proceed step-by-step, incorporating one parameter at a time to understand the importance of each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_excitation(num_samples, v_voiced, v_f0, fs):\n",
    "    \"\"\"\n",
    "    Generate excitation signal based on voiced/unvoiced decision and f0.\n",
    "    \n",
    "    Parameters:\n",
    "        num_samples: Number of samples in the output signal\n",
    "        v_voiced: Vector of voiced/unvoiced decisions\n",
    "        v_f0: Vector of fundamental frequency estimates\n",
    "        fs: Sampling frequency in Hz\n",
    "        \n",
    "    Returns:\n",
    "        excitation: Excitation signal\n",
    "    \"\"\"\n",
    "    # Initialize excitation signal\n",
    "    excitation = np.zeros(num_samples)\n",
    "    \n",
    "    # Frame parameters\n",
    "    frame_length = int((frame_length_ms / 1000) * fs)\n",
    "    frame_shift = int((frame_shift_ms / 1000) * fs)\n",
    "    \n",
    "    # Generate excitation for each frame\n",
    "    for i in range(len(v_voiced)):\n",
    "        # Frame boundaries\n",
    "        start = i * frame_shift\n",
    "        end = min(start + frame_length, num_samples)\n",
    "        \n",
    "        if v_voiced[i] == 1:  # Voiced\n",
    "            # Compute period in samples\n",
    "            period = int(fs / v_f0[i]) if v_f0[i] > 0 else 0\n",
    "            \n",
    "            if period > 0:\n",
    "                # Generate pulse train\n",
    "                for j in range(start, end):\n",
    "                    if (j % period) == 0:\n",
    "                        excitation[j] = 1.0\n",
    "        else:  # Unvoiced\n",
    "            # Generate white noise\n",
    "            excitation[start:end] = np.random.randn(end - start)\n",
    "    \n",
    "    return excitation\n",
    "\n",
    "def synthesize_speech(v_voiced, v_f0, m_lpc, v_power, fs, frame_length_ms, frame_shift_ms):\n",
    "    \"\"\"\n",
    "    Synthesize speech using LPC vocoder.\n",
    "    \n",
    "    Parameters:\n",
    "        v_voiced: Vector of voiced/unvoiced decisions\n",
    "        v_f0: Vector of fundamental frequency estimates\n",
    "        m_lpc: Matrix of LP coefficients\n",
    "        v_power: Vector of frame powers\n",
    "        fs: Sampling frequency in Hz\n",
    "        frame_length_ms: Frame length in milliseconds\n",
    "        frame_shift_ms: Frame shift in milliseconds\n",
    "        \n",
    "    Returns:\n",
    "        synthesized: Synthesized speech signal\n",
    "    \"\"\"\n",
    "    # Frame parameters\n",
    "    frame_length = int((frame_length_ms / 1000) * fs)\n",
    "    frame_shift = int((frame_shift_ms / 1000) * fs)\n",
    "    \n",
    "    # Calculate total number of samples\n",
    "    num_frames = len(v_voiced)\n",
    "    num_samples = (num_frames - 1) * frame_shift + frame_length\n",
    "    \n",
    "    # Generate excitation signal\n",
    "    excitation = generate_excitation(num_samples, v_voiced, v_f0, fs)\n",
    "    \n",
    "    # Initialize synthesized signal\n",
    "    synthesized = np.zeros(num_samples)\n",
    "    \n",
    "    # Synthesize speech frame by frame\n",
    "    for i in range(num_frames):\n",
    "        # Frame boundaries\n",
    "        start = i * frame_shift\n",
    "        end = min(start + frame_length, num_samples)\n",
    "        \n",
    "        # Get LPC coefficients for this frame\n",
    "        a = np.concatenate(([1.0], -m_lpc[i]))  # Add 1.0 at the beginning and negate\n",
    "        \n",
    "        # Get excitation for this frame\n",
    "        frame_excitation = excitation[start:end]\n",
    "        \n",
    "        # Scale excitation by power\n",
    "        gain = np.sqrt(v_power[i])\n",
    "        frame_excitation = frame_excitation * gain\n",
    "        \n",
    "        # Apply LPC filter\n",
    "        frame_synthesized = lfilter([1.0], a, frame_excitation)\n",
    "        \n",
    "        # Add to output with overlap-add\n",
    "        synthesized[start:end] += frame_synthesized\n",
    "    \n",
    "    return synthesized\n",
    "\n",
    "# Synthesize speech\n",
    "synthesized = synthesize_speech(v_voiced, v_f0, m_lpc, v_power, fs, frame_length_ms, frame_shift_ms)\n",
    "\n",
    "# Normalize to prevent clipping\n",
    "synthesized = synthesized / np.max(np.abs(synthesized))\n",
    "\n",
    "# Plot original and synthesized signals\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(time, x)\n",
    "plt.title('Original Speech Signal')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "synth_time = np.arange(len(synthesized)) / fs\n",
    "plt.plot(synth_time, synthesized)\n",
    "plt.title('Synthesized Speech Signal')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Play original and synthesized signals\n",
    "print(\"Playing original signal...\")\n",
    "sd.play(x, fs)\n",
    "sd.wait()\n",
    "\n",
    "print(\"Playing synthesized signal...\")\n",
    "sd.play(synthesized, fs)\n",
    "sd.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain the importance of incorporating signal power in the synthesis process.**\n",
    "\n",
    "Incorporating signal power in the synthesis process is crucial for several reasons:\n",
    "\n",
    "1. **Amplitude modulation**: Speech signals naturally vary in amplitude over time. Without power information, the synthesized speech would have a constant energy level, resulting in unnatural-sounding speech that lacks the dynamic range of natural speech.\n",
    "\n",
    "2. **Perceptual quality**: The human auditory system is sensitive to intensity variations in speech. These variations convey important prosodic information such as stress, emphasis, and emotional content. Incorporating power helps preserve these perceptual cues.\n",
    "\n",
    "3. **Phonetic distinctions**: Different phonemes have inherently different energy levels. For example, vowels typically have higher energy than consonants. Preserving these energy differences is essential for maintaining phonetic distinctions in the synthesized speech.\n",
    "\n",
    "4. **Voiced/unvoiced transitions**: The transitions between voiced and unvoiced segments often involve changes in signal power. Properly modeling these transitions improves the naturalness of the synthesized speech.\n",
    "\n",
    "5. **Signal-to-noise ratio**: Scaling the excitation signal by the appropriate gain ensures that the synthesized speech has a similar signal-to-noise ratio as the original speech, which is important for intelligibility.\n",
    "\n",
    "In our implementation, we incorporate power by scaling the excitation signal for each frame by the square root of the frame power (which gives us the standard deviation). This ensures that the energy of the synthesized speech matches that of the original speech on a frame-by-frame basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parameter Quantization\n",
    "\n",
    "In practical speech coding applications, the LPC parameters need to be quantized for efficient transmission. Let's explore different quantization strategies for the LPC coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert LPC coefficients to reflection coefficients (RCs)\n",
    "m_rc = np.zeros_like(m_lpc)\n",
    "for i in range(len(m_lpc)):\n",
    "    m_rc[i] = poly2rc(np.concatenate(([1.0], -m_lpc[i])))\n",
    "\n",
    "# Convert reflection coefficients to log area ratios (LARs)\n",
    "m_lar = np.zeros_like(m_rc)\n",
    "for i in range(len(m_rc)):\n",
    "    m_lar[i] = rc2lar(m_rc[i])\n",
    "\n",
    "# Plot the different representations for a voiced frame\n",
    "voiced_frame_idx = np.where(v_voiced == 1)[0][10]  # Choose a voiced frame\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.stem(m_lpc[voiced_frame_idx])\n",
    "plt.title(f'LPC Coefficients for Frame {voiced_frame_idx}')\n",
    "plt.xlabel('Coefficient Index')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.stem(m_rc[voiced_frame_idx])\n",
    "plt.title(f'Reflection Coefficients for Frame {voiced_frame_idx}')\n",
    "plt.xlabel('Coefficient Index')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.stem(m_lar[voiced_frame_idx])\n",
    "plt.title(f'Log Area Ratios for Frame {voiced_frame_idx}')\n",
    "plt.xlabel('Coefficient Index')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_uniform(data, bits):\n",
    "    \"\"\"\n",
    "    Uniformly quantize data using the specified number of bits.\n",
    "    \n",
    "    Parameters:\n",
    "        data: Input data to quantize\n",
    "        bits: Number of bits for quantization\n",
    "        \n",
    "    Returns:\n",
    "        quantized: Quantized data\n",
    "    \"\"\"\n",
    "    # Determine range of data\n",
    "    data_min = np.min(data)\n",
    "    data_max = np.max(data)\n",
    "    \n",
    "    # Number of quantization levels\n",
    "    levels = 2**bits\n",
    "    \n",
    "    # Quantization step size\n",
    "    step = (data_max - data_min) / (levels - 1)\n",
    "    \n",
    "    # Quantize\n",
    "    quantized_indices = np.round((data - data_min) / step).astype(int)\n",
    "    quantized = data_min + quantized_indices * step\n",
    "    \n",
    "    return quantized\n",
    "\n",
    "# Quantize LPC, RC, and LAR representations with different bit rates\n",
    "bits = 4  # Number of bits per coefficient\n",
    "\n",
    "# Quantize each coefficient separately\n",
    "m_lpc_quant = np.zeros_like(m_lpc)\n",
    "m_rc_quant = np.zeros_like(m_rc)\n",
    "m_lar_quant = np.zeros_like(m_lar)\n",
    "\n",
    "for j in range(M):\n",
    "    m_lpc_quant[:, j] = quantize_uniform(m_lpc[:, j], bits)\n",
    "    m_rc_quant[:, j] = quantize_uniform(m_rc[:, j], bits)\n",
    "    m_lar_quant[:, j] = quantize_uniform(m_lar[:, j], bits)\n",
    "\n",
    "# Convert quantized parameters back to LPC for synthesis\n",
    "m_lpc_from_rc = np.zeros_like(m_lpc)\n",
    "m_lpc_from_lar = np.zeros_like(m_lpc)\n",
    "\n",
    "for i in range(len(m_rc_quant)):\n",
    "    # RC to LPC\n",
    "    a_rc = rc2poly(m_rc_quant[i])\n",
    "    m_lpc_from_rc[i] = -a_rc[1:]\n",
    "    \n",
    "    # LAR to LPC\n",
    "    rc_lar = lar2rc(m_lar_quant[i])\n",
    "    a_lar = rc2poly(rc_lar)\n",
    "    m_lpc_from_lar[i] = -a_lar[1:]\n",
    "\n",
    "# Synthesize speech using quantized parameters\n",
    "synthesized_lpc_quant = synthesize_speech(v_voiced, v_f0, m_lpc_quant, v_power, fs, frame_length_ms, frame_shift_ms)\n",
    "synthesized_rc_quant = synthesize_speech(v_voiced, v_f0, m_lpc_from_rc, v_power, fs, frame_length_ms, frame_shift_ms)\n",
    "synthesized_lar_quant = synthesize_speech(v_voiced, v_f0, m_lpc_from_lar, v_power, fs, frame_length_ms, frame_shift_ms)\n",
    "\n",
    "# Normalize\n",
    "synthesized_lpc_quant = synthesized_lpc_quant / np.max(np.abs(synthesized_lpc_quant))\n",
    "synthesized_rc_quant = synthesized_rc_quant / np.max(np.abs(synthesized_rc_quant))\n",
    "synthesized_lar_quant = synthesized_lar_quant / np.max(np.abs(synthesized_lar_quant))\n",
    "\n",
    "# Play synthesized signals\n",
    "print(f\"Playing synthesized signal with quantized LPC ({bits} bits per coefficient)...\")\n",
    "sd.play(synthesized_lpc_quant, fs)\n",
    "sd.wait()\n",
    "\n",
    "print(f\"Playing synthesized signal with quantized RC ({bits} bits per coefficient)...\")\n",
    "sd.play(synthesized_rc_quant, fs)\n",
    "sd.wait()\n",
    "\n",
    "print(f\"Playing synthesized signal with quantized LAR ({bits} bits per coefficient)...\")\n",
    "sd.play(synthesized_lar_quant, fs)\n",
    "sd.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare the different parameter representations (LPC, RC, LAR) in terms of their suitability for quantization.**\n",
    "\n",
    "The three parameter representations (LPC, RC, LAR) have different properties that affect their suitability for quantization:\n",
    "\n",
    "1. **Linear Prediction Coefficients (LPC)**:\n",
    "   - **Range**: Unbounded and can vary widely\n",
    "   - **Distribution**: Often non-uniform\n",
    "   - **Stability**: Quantization can easily lead to unstable filters (poles outside the unit circle)\n",
    "   - **Sensitivity**: Small changes in coefficients can cause large changes in spectral characteristics\n",
    "   - **Suitability for quantization**: Poor - direct quantization of LPC coefficients is generally avoided in speech coding\n",
    "\n",
    "2. **Reflection Coefficients (RC)**:\n",
    "   - **Range**: Bounded between -1 and 1\n",
    "   - **Distribution**: More uniform than LPC coefficients\n",
    "   - **Stability**: Filter stability is guaranteed as long as |RC| < 1 (which is preserved during quantization)\n",
    "   - **Sensitivity**: Less sensitive to quantization errors than LPC coefficients\n",
    "   - **Suitability for quantization**: Good - widely used in speech coding systems\n",
    "\n",
    "3. **Log Area Ratios (LAR)**:\n",
    "   - **Range**: Unbounded, but with a more predictable distribution\n",
    "   - **Distribution**: More uniform and symmetric around zero\n",
    "   - **Stability**: Filter stability is preserved as long as finite LAR values are maintained\n",
    "   - **Sensitivity**: Even less sensitive to quantization errors, especially for coefficients close to Â±1\n",
    "   - **Suitability for quantization**: Excellent - particularly good for low bit-rate coding\n",
    "\n",
    "Based on our experiments with 4-bit quantization:\n",
    "\n",
    "- The LPC-quantized speech showed the most distortion, with potential instability issues.\n",
    "- The RC-quantized speech maintained better quality, with guaranteed stability.\n",
    "- The LAR-quantized speech provided the best perceptual quality, especially for the lower-order coefficients which have the most impact on the spectral envelope.\n",
    "\n",
    "In practical speech coding systems, LAR or similar transformations (like Line Spectral Frequencies/LSF) are preferred for quantization because they provide a good balance between bit rate efficiency and speech quality. They allow for non-uniform quantization that allocates more bits to perceptually important coefficients and fewer bits to less important ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "In this exercise, we've built a complete LPC-vocoder system, including:\n",
    "\n",
    "1. **Analysis**: Extracting parameters from the speech signal\n",
    "   - Signal segmentation\n",
    "   - Power estimation\n",
    "   - Voiced/unvoiced decision\n",
    "   - Fundamental frequency estimation\n",
    "   - Linear prediction coefficient computation\n",
    "\n",
    "2. **Synthesis**: Reconstructing speech from the extracted parameters\n",
    "   - Excitation generation (pulse train for voiced, noise for unvoiced)\n",
    "   - LPC filtering\n",
    "   - Overlap-add synthesis\n",
    "\n",
    "3. **Parameter Quantization**: Exploring different representations for efficient coding\n",
    "   - LPC coefficients\n",
    "   - Reflection coefficients\n",
    "   - Log area ratios\n",
    "\n",
    "This exercise demonstrates the fundamental principles behind many speech coding standards used in telecommunications, including GSM, AMR, and others. The LPC-vocoder approach allows for very low bit-rate transmission of speech (as low as 2.4 kbps) while maintaining reasonable intelligibility.\n",
    "\n",
    "Modern speech coders build upon these principles with additional techniques such as:\n",
    "- Vector quantization of parameters\n",
    "- More sophisticated excitation models (e.g., CELP - Code Excited Linear Prediction)\n",
    "- Perceptual weighting to mask quantization noise\n",
    "- Long-term prediction for better modeling of pitch periodicity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
